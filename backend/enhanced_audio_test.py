"""
Enhanced audio processing and Whisper transcription
This version handles audio extraction more robustly
"""

import whisper
import subprocess
import tempfile
import os
from pathlib import Path
import json

class AudioProcessor:
    def __init__(self):
        self.model = None
        self.temp_dir = tempfile.mkdtemp()
    
    def load_whisper_model(self, model_size="base"):
        """Load Whisper model"""
        print(f"ğŸ¤ Loading Whisper model: {model_size}")
        try:
            self.model = whisper.load_model(model_size)
            print("âœ… Whisper model loaded successfully!")
            return True
        except Exception as e:
            print(f"âŒ Error loading Whisper model: {e}")
            return False
    
    def extract_audio_with_opencv(self, video_path):
        """Extract audio using OpenCV and save as WAV"""
        try:
            import cv2
            
            # Try to extract audio info from video
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                return None
                
            # For now, we'll transcribe directly from video
            # OpenCV doesn't handle audio extraction well
            cap.release()
            return str(video_path)  # Return video path for direct processing
            
        except Exception as e:
            print(f"Audio extraction error: {e}")
            return None
    
    def transcribe_video(self, video_path):
        """Transcribe video using Whisper directly"""
        if not self.model:
            print("âŒ Whisper model not loaded")
            return None
        
        try:
            print(f"ğŸ¤ Transcribing: {Path(video_path).name}")
            
            # Use Whisper to transcribe directly from video
            result = self.model.transcribe(str(video_path), fp16=False)
            
            return {
                "text": result.get("text", "").strip(),
                "language": result.get("language", "unknown"),
                "segments": result.get("segments", []),
                "success": True
            }
            
        except Exception as e:
            print(f"âŒ Transcription error: {e}")
            return {
                "text": "",
                "language": "unknown", 
                "segments": [],
                "success": False,
                "error": str(e)
            }
    
    def analyze_transcript(self, transcript_result):
        """Analyze transcript for insights"""
        if not transcript_result or not transcript_result.get("success"):
            return {"analysis": "No audio content detected or transcription failed"}
        
        text = transcript_result["text"]
        segments = transcript_result.get("segments", [])
        
        if not text.strip():
            return {"analysis": "No speech detected in audio"}
        
        # Basic analysis
        word_count = len(text.split())
        duration = segments[-1]["end"] if segments else 0
        speaking_rate = word_count / (duration / 60) if duration > 0 else 0
        
        # Detect content type
        content_indicators = {
            "presentation": ["slide", "next", "show", "demonstrate", "example"],
            "conversation": ["um", "uh", "you know", "like", "actually"],
            "lecture": ["today we", "let's discuss", "important", "concept"],
            "tutorial": ["first", "step", "how to", "follow", "guide"]
        }
        
        text_lower = text.lower()
        content_scores = {}
        for content_type, keywords in content_indicators.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            content_scores[content_type] = score
        
        likely_content = max(content_scores, key=content_scores.get) if content_scores else "general"
        
        return {
            "word_count": word_count,
            "duration_minutes": round(duration / 60, 2),
            "speaking_rate_wpm": round(speaking_rate, 1),
            "likely_content_type": likely_content,
            "language": transcript_result.get("language", "unknown"),
            "has_speech": bool(text.strip())
        }

def test_enhanced_transcription():
    """Test enhanced transcription capabilities"""
    print("ğŸ¤ Enhanced Audio Processing Test")
    print("="*50)
    
    processor = AudioProcessor()
    
    # Load Whisper model
    if not processor.load_whisper_model("base"):
        print("âŒ Cannot proceed without Whisper model")
        return
    
    # Find video files - use absolute path
    current_dir = Path(__file__).parent
    video_dir = current_dir.parent / "data" / "videos"
    print(f"ğŸ” Looking for videos in: {video_dir}")
    
    if not video_dir.exists():
        print(f"âŒ Video directory not found: {video_dir}")
        return
    
    video_files = [f for f in video_dir.glob("*.mp4") if f.is_file()]
    
    if not video_files:
        print("âŒ No video files found")
        return
    
    results = []
    
    for video_file in video_files:
        print(f"\nğŸ“¹ Processing: {video_file.name}")
        
        # Transcribe
        transcript_result = processor.transcribe_video(video_file)
        
        if transcript_result and transcript_result.get("success"):
            print("âœ… Transcription successful!")
            print(f"Text: {transcript_result['text'][:100]}...")
            
            # Analyze transcript
            analysis = processor.analyze_transcript(transcript_result)
            
            result = {
                "file": video_file.name,
                "transcript": transcript_result,
                "analysis": analysis
            }
            results.append(result)
            
        else:
            print("âŒ Transcription failed")
            error_msg = transcript_result.get("error", "Unknown error") if transcript_result else "No result"
            results.append({
                "file": video_file.name,
                "transcript": {"success": False, "error": error_msg},
                "analysis": {"has_speech": False}
            })
    
    # Generate comprehensive report
    generate_audio_report(results)
    
    return results

def generate_audio_report(results):
    """Generate comprehensive audio analysis report"""
    report = """
SVA - AUDIO ANALYSIS REPORT
===========================

"""
    
    for i, result in enumerate(results, 1):
        file_name = result["file"]
        transcript = result["transcript"]
        analysis = result["analysis"]
        
        report += f"""
{i}. AUDIO ANALYSIS: {file_name}
{'='*50}

TRANSCRIPTION STATUS: {'âœ… SUCCESS' if transcript.get('success') else 'âŒ FAILED'}
"""
        
        if transcript.get("success"):
            text = transcript["text"]
            language = transcript.get("language", "unknown")
            
            report += f"""
DETECTED LANGUAGE: {language.upper()}

FULL TRANSCRIPT:
{text}

ANALYSIS:
- Word Count: {analysis.get('word_count', 0)}
- Duration: {analysis.get('duration_minutes', 0)} minutes
- Speaking Rate: {analysis.get('speaking_rate_wpm', 0)} words/minute
- Content Type: {analysis.get('likely_content_type', 'unknown').title()}
- Has Speech: {'Yes' if analysis.get('has_speech') else 'No'}

"""
            
            # Add segment details if available
            segments = transcript.get("segments", [])
            if segments:
                report += "DETAILED SEGMENTS:\n"
                for segment in segments[:5]:  # First 5 segments
                    start = segment.get('start', 0)
                    end = segment.get('end', 0)
                    text = segment.get('text', '').strip()
                    report += f"[{start:.1f}s - {end:.1f}s]: {text}\n"
                
                if len(segments) > 5:
                    report += f"... and {len(segments) - 5} more segments\n"
        
        else:
            error = transcript.get("error", "Unknown error")
            report += f"""
ERROR: {error}

POSSIBLE CAUSES:
- Video has no audio track
- Audio format not supported
- File corruption
- FFmpeg not properly configured

"""
    
    report += """

SUMMARY & NEXT STEPS:
====================
âœ… Whisper AI transcription: IMPLEMENTED
âœ… Multi-language support: AVAILABLE
âœ… Segment-level timestamps: WORKING
âœ… Content type detection: BASIC ANALYSIS

CAPABILITIES UNLOCKED:
ğŸ¤ Speech-to-text conversion
ğŸŒ Multi-language detection
â±ï¸ Timestamp-accurate segments
ğŸ“Š Speaking rate analysis
ğŸ¯ Content classification

NEXT PHASE: Vision AI & Object Detection
"""
    
    # Save report
    with open("audio_analysis_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\nâœ… Audio analysis report saved: audio_analysis_report.txt")

if __name__ == "__main__":
    test_enhanced_transcription()