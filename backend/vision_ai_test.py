"""
Phase 2: Vision AI & Object Detection
This implements visual analysis capabilities while we get proper audio samples
"""

import cv2
import numpy as np
from pathlib import Path
import json

class VisionAnalyzer:
    def __init__(self):
        self.face_cascade = None
        self.load_opencv_models()
    
    def load_opencv_models(self):
        """Load OpenCV's built-in models"""
        try:
            # Load face detection (comes with OpenCV)
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            print("‚úÖ OpenCV face detection loaded")
        except Exception as e:
            print(f"‚ö†Ô∏è  Face detection not available: {e}")
    
    def detect_faces(self, frame):
        """Detect faces in frame"""
        if self.face_cascade is None:
            return []
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        return faces
    
    def analyze_colors(self, frame):
        """Analyze color distribution in frame"""
        # Convert to different color spaces
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # Calculate color statistics
        bgr_mean = np.mean(frame, axis=(0, 1))
        hsv_mean = np.mean(hsv, axis=(0, 1))
        
        # Dominant color analysis
        dominant_color = self.get_dominant_color(frame)
        
        return {
            "bgr_mean": bgr_mean.tolist(),
            "hsv_mean": hsv_mean.tolist(), 
            "dominant_color": dominant_color,
            "brightness": float(np.mean(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))),
            "contrast": float(np.std(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)))
        }
    
    def get_dominant_color(self, frame, k=3):
        """Get dominant colors using k-means clustering"""
        # Reshape frame to be a list of pixels
        pixels = frame.reshape((-1, 3))
        pixels = np.float32(pixels)
        
        # Apply k-means clustering
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        
        # Convert back to uint8 and return dominant color
        centers = np.uint8(centers)
        dominant = centers[0]  # Most dominant cluster center
        
        return dominant.tolist()
    
    def detect_edges(self, frame):
        """Detect edges and analyze image complexity"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        
        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
        
        return {
            "edge_density": float(edge_density),
            "complexity": "high" if edge_density > 0.1 else "medium" if edge_density > 0.05 else "low"
        }
    
    def detect_text_regions(self, frame):
        """Detect potential text regions"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Use morphological operations to find text-like regions
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        grad = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT, kernel)
        
        # Threshold and find contours
        _, thresh = cv2.threshold(grad, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Filter contours that might be text
        text_regions = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            aspect_ratio = w / h if h > 0 else 0
            area = cv2.contourArea(contour)
            
            # Text-like characteristics: reasonable aspect ratio and area
            if 0.2 < aspect_ratio < 5.0 and 100 < area < 10000:
                text_regions.append({
                    "x": int(x), "y": int(y), "w": int(w), "h": int(h),
                    "area": float(area), "aspect_ratio": float(aspect_ratio)
                })
        
        return text_regions
    
    def analyze_motion_between_frames(self, frame1, frame2):
        """Analyze motion between two frames using frame difference"""
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        
        # Calculate frame difference (more reliable than optical flow)
        diff = cv2.absdiff(gray1, gray2)
        motion_score = np.mean(diff)
        
        # Calculate motion statistics
        motion_pixels = np.sum(diff > 30)  # Pixels with significant change
        total_pixels = diff.shape[0] * diff.shape[1]
        motion_percentage = motion_pixels / total_pixels
        
        return {
            "motion_score": float(motion_score),
            "motion_percentage": float(motion_percentage),
            "motion_level": "high" if motion_score > 20 else "medium" if motion_score > 5 else "low"
        }

def comprehensive_vision_analysis(video_path):
    """Perform comprehensive vision analysis on video"""
    print(f"üëÅÔ∏è  Vision Analysis: {Path(video_path).name}")
    
    analyzer = VisionAnalyzer()
    cap = cv2.VideoCapture(str(video_path))
    
    if not cap.isOpened():
        return {"error": "Cannot open video"}
    
    # Video metadata
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps if fps > 0 else 0
    
    # Analysis results
    results = {
        "file": Path(video_path).name,
        "metadata": {
            "duration": duration,
            "fps": fps,
            "total_frames": int(frame_count)
        },
        "visual_analysis": {
            "faces_detected": [],
            "colors": [],
            "motion": [],
            "text_regions": [],
            "scene_types": []
        }
    }
    
    frame_num = 0
    prev_frame = None
    sample_interval = max(1, int(fps))  # Sample every second
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if frame_num % sample_interval == 0:
            timestamp = frame_num / fps
            print(f"  Analyzing frame at {timestamp:.1f}s...")
            
            # Face detection
            faces = analyzer.detect_faces(frame)
            if len(faces) > 0:
                results["visual_analysis"]["faces_detected"].append({
                    "timestamp": timestamp,
                    "face_count": len(faces),
                    "faces": [{"x": int(x), "y": int(y), "w": int(w), "h": int(h)} 
                             for (x, y, w, h) in faces]
                })
            
            # Color analysis
            color_analysis = analyzer.analyze_colors(frame)
            color_analysis["timestamp"] = timestamp
            results["visual_analysis"]["colors"].append(color_analysis)
            
            # Edge detection
            edge_analysis = analyzer.detect_edges(frame)
            edge_analysis["timestamp"] = timestamp
            
            # Text region detection
            text_regions = analyzer.detect_text_regions(frame)
            if text_regions:
                results["visual_analysis"]["text_regions"].append({
                    "timestamp": timestamp,
                    "regions": text_regions
                })
            
            # Motion analysis (if we have previous frame)
            if prev_frame is not None:
                motion_analysis = analyzer.analyze_motion_between_frames(prev_frame, frame)
                motion_analysis["timestamp"] = timestamp
                results["visual_analysis"]["motion"].append(motion_analysis)
            
            prev_frame = frame.copy()
        
        frame_num += 1
    
    cap.release()
    
    # Generate insights
    results["insights"] = generate_vision_insights(results)
    
    return results

def generate_vision_insights(analysis_results):
    """Generate insights from vision analysis"""
    colors = analysis_results["visual_analysis"]["colors"]
    faces = analysis_results["visual_analysis"]["faces_detected"]
    motion = analysis_results["visual_analysis"]["motion"]
    text_regions = analysis_results["visual_analysis"]["text_regions"]
    
    insights = {}
    
    # Face analysis insights
    if faces:
        face_timestamps = [f["timestamp"] for f in faces]
        total_faces = sum(f["face_count"] for f in faces)
        insights["people"] = {
            "people_detected": True,
            "face_appearances": len(faces),
            "total_face_detections": total_faces,
            "first_appearance": min(face_timestamps),
            "last_appearance": max(face_timestamps)
        }
    else:
        insights["people"] = {"people_detected": False}
    
    # Color insights
    if colors:
        avg_brightness = np.mean([c["brightness"] for c in colors])
        brightness_std = np.std([c["brightness"] for c in colors])
        
        insights["visual_quality"] = {
            "average_brightness": float(avg_brightness),
            "brightness_variation": float(brightness_std),
            "lighting": "bright" if avg_brightness > 120 else "medium" if avg_brightness > 80 else "dark",
            "stability": "stable" if brightness_std < 20 else "variable"
        }
    
    # Motion insights
    if motion:
        avg_motion = np.mean([m["motion_score"] for m in motion])
        high_motion_frames = len([m for m in motion if m["motion_level"] == "high"])
        
        insights["activity"] = {
            "average_motion": float(avg_motion),
            "high_activity_frames": high_motion_frames,
            "video_type": "dynamic" if avg_motion > 15 else "static"
        }
    
    # Text insights
    insights["text_content"] = {
        "text_regions_detected": len(text_regions),
        "likely_has_text": len(text_regions) > 0,
        "text_timestamps": [tr["timestamp"] for tr in text_regions]
    }
    
    # Overall classification
    if insights.get("people", {}).get("people_detected"):
        if insights.get("activity", {}).get("video_type") == "dynamic":
            video_type = "interview_or_presentation"
        else:
            video_type = "talking_head_or_portrait"
    elif insights.get("text_content", {}).get("likely_has_text"):
        video_type = "educational_or_tutorial"
    elif insights.get("activity", {}).get("video_type") == "dynamic":
        video_type = "action_or_demo"
    else:
        video_type = "scenic_or_static"
    
    insights["classification"] = {
        "likely_content_type": video_type,
        "confidence": "medium"  # Could be improved with more sophisticated analysis
    }
    
    return insights

def test_vision_analysis():
    """Test vision analysis on available videos"""
    print("üëÅÔ∏è  COMPREHENSIVE VISION ANALYSIS")
    print("="*50)
    
    video_dir = Path("../data/videos")
    video_files = [f for f in video_dir.glob("*.mp4") if f.is_file()]
    
    if not video_files:
        print("‚ùå No videos found")
        return
    
    all_results = []
    
    for video_file in video_files:
        try:
            result = comprehensive_vision_analysis(video_file)
            all_results.append(result)
        except Exception as e:
            print(f"‚ùå Error analyzing {video_file.name}: {e}")
            all_results.append({"error": str(e), "file": video_file.name})
    
    # Generate report
    generate_vision_report(all_results)
    
    return all_results

def generate_vision_report(results):
    """Generate comprehensive vision analysis report"""
    report = """
SVA - VISION AI ANALYSIS REPORT
===============================

"""
    
    for i, result in enumerate(results, 1):
        if "error" in result:
            report += f"\n{i}. ERROR: {result['file']} - {result['error']}\n"
            continue
        
        file_name = result["file"]
        insights = result["insights"]
        
        report += f"""
{i}. VISION ANALYSIS: {file_name}
{'='*50}

CONTENT CLASSIFICATION:
- Type: {insights.get('classification', {}).get('likely_content_type', 'unknown').replace('_', ' ').title()}
- Confidence: {insights.get('classification', {}).get('confidence', 'unknown')}

PEOPLE DETECTION:
- People Present: {'‚úÖ' if insights.get('people', {}).get('people_detected') else '‚ùå'}
"""
        
        if insights.get('people', {}).get('people_detected'):
            people_info = insights['people']
            report += f"""- Face Detections: {people_info.get('total_face_detections', 0)}
- First Appearance: {people_info.get('first_appearance', 0):.1f}s
- Last Appearance: {people_info.get('last_appearance', 0):.1f}s
"""
        
        if 'visual_quality' in insights:
            vq = insights['visual_quality']
            report += f"""
VISUAL QUALITY:
- Lighting: {vq.get('lighting', 'unknown').title()}
- Brightness: {vq.get('average_brightness', 0):.1f}
- Stability: {vq.get('stability', 'unknown').title()}
"""
        
        if 'activity' in insights:
            activity = insights['activity']
            report += f"""
ACTIVITY ANALYSIS:
- Video Type: {activity.get('video_type', 'unknown').title()}
- Motion Level: {activity.get('average_motion', 0):.1f}
- Dynamic Frames: {activity.get('high_activity_frames', 0)}
"""
        
        if 'text_content' in insights:
            text_info = insights['text_content']
            report += f"""
TEXT DETECTION:
- Text Regions Found: {text_info.get('text_regions_detected', 0)}
- Likely Has Text: {'‚úÖ' if text_info.get('likely_has_text') else '‚ùå'}
"""
    
    report += """

VISION AI CAPABILITIES DEMONSTRATED:
====================================
‚úÖ Face Detection: Identifies people in videos
‚úÖ Color Analysis: Analyzes lighting and visual quality  
‚úÖ Motion Detection: Measures activity and movement
‚úÖ Text Region Detection: Finds potential text areas
‚úÖ Scene Classification: Categorizes content type
‚úÖ Temporal Analysis: Tracks changes over time

READY FOR INTEGRATION:
üîÑ Can be combined with audio transcription
üí¨ Ready for natural language queries
üìä Can generate detailed reports
üéØ Suitable for content analysis workflows

PHASE 2 STATUS: VISION AI COMPLETE! üëÅÔ∏è‚úÖ
"""
    
    # Save report
    with open("vision_analysis_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\n‚úÖ Vision analysis report saved: vision_analysis_report.txt")

if __name__ == "__main__":
    test_vision_analysis()